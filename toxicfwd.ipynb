{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from fastai.text import *\n",
    "import html\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "DATA_PATH=Path('/scratch/ppachigo/3192771/fastai/data/')    ## edit path\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "#! curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "#! tar -xzfv aclImdb_v1.tar.gz -C {DATA_PATH}\n",
    "\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "PATH=Path('/scratch/ppachigo/3192771/fastai/data/toxic/')\n",
    "\n",
    "CLAS_PATH=Path('/scratch/ppachigo/3192771/fastai/data/toxic_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "LM_PATH=Path('/scratch/ppachigo/3192771/fastai/data/toxic_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "#CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "#def get_texts(path):\n",
    "#    texts,labels = [],[]\n",
    "#    for idx,label in enumerate(CLASSES):\n",
    "#        print(idx,label)\n",
    "#        for fname in (path/label).glob('*.*'):\n",
    "#            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "#            labels.append(idx)\n",
    "#    return np.array(texts),np.array(labels)\n",
    "\n",
    "trn_ds = pd.read_csv(PATH/'train.csv')\n",
    "val_ds = pd.read_csv(PATH/'test.csv')\n",
    "vallabelsds = pd.read_csv(PATH/'test_labels.csv')\n",
    "\n",
    "val_dswlabels = pd.merge(val_ds, vallabelsds, how = 'left', left_on = 'id', right_on = 'id')\n",
    "\n",
    "trn_texts = np.array(trn_ds['comment_text'])\n",
    "val_texts = np.array(val_dswlabels['comment_text'])\n",
    "\n",
    "\n",
    "###\n",
    "trn_labels_tx = np.array(trn_ds['toxic'])\n",
    "trn_labels_stx = np.array(trn_ds['severe_toxic'])\n",
    "trn_labels_obs = np.array(trn_ds['obscene'])\n",
    "trn_labels_thrt = np.array(trn_ds['threat'])\n",
    "trn_labels_ins = np.array(trn_ds['insult'])\n",
    "trn_labels_idthr = np.array(trn_ds['identity_hate'])\n",
    "\n",
    "\n",
    "val_labels_tx = np.array(val_dswlabels['toxic'])\n",
    "val_labels_stx = np.array(val_dswlabels['severe_toxic'])\n",
    "val_labels_obs = np.array(val_dswlabels['obscene'])\n",
    "val_labels_thrt = np.array(val_dswlabels['threat'])\n",
    "val_labels_ins = np.array(val_dswlabels['insult'])\n",
    "val_labels_idthr = np.array(val_dswlabels['identity_hate'])\n",
    "\n",
    "#val_labels = np.array(val_dswlabels[['toxic','severe_toxic','obscene','threat','insult','identity_hate']])\n",
    "\n",
    "trn_ids = np.array(trn_ds['id'])\n",
    "val_ids = np.array(val_dswlabels['id'])\n",
    "\n",
    "\n",
    "#trn_texts,trn_labels = get_texts(PATH/'train')\n",
    "#val_texts,val_labels = get_texts(PATH/'test')\n",
    "\n",
    "print('len trn txts', len(trn_texts))\n",
    "print('len val texts',len(val_texts))\n",
    "\n",
    "col_names = ['toxic','severe_toxic','obscene','threat','insult','identity_hate','text']\n",
    "\n",
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "\n",
    "val_idx = np.random.permutation(len(val_texts))\n",
    "\n",
    "\n",
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "\n",
    "#trn_labels = trn_labels[trn_idx]\n",
    "#val_labels = val_labels[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "trn_labels_tx = trn_labels_tx[trn_idx]\n",
    "trn_labels_stx = trn_labels_stx[trn_idx]\n",
    "trn_labels_obs = trn_labels_obs[trn_idx]\n",
    "trn_labels_thrt = trn_labels_thrt[trn_idx]\n",
    "trn_labels_ins = trn_labels_ins[trn_idx]\n",
    "trn_labels_idthr = trn_labels_idthr[trn_idx]\n",
    "\n",
    "\n",
    "val_labels_tx = val_labels_tx[val_idx]\n",
    "val_labels_stx = val_labels_stx[val_idx]\n",
    "val_labels_obs = val_labels_obs[val_idx]\n",
    "val_labels_thrt = val_labels_thrt[val_idx]\n",
    "val_labels_ins = val_labels_ins[val_idx]\n",
    "val_labels_idthr = val_labels_idthr[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trn_ids = trn_ids[trn_idx]\n",
    "val_ids = val_ids[val_idx]\n",
    "\n",
    "\n",
    "df_trn = pd.DataFrame({'toxic':trn_labels_tx, 'severe_toxic': trn_labels_stx, 'obscene': trn_labels_obs, 'threat': trn_labels_thrt, 'insult': trn_labels_ins,'identity_hate':trn_labels_idthr,'text':trn_texts,}, columns=col_names)\n",
    "df_val = pd.DataFrame({ 'toxic':val_labels_tx, 'severe_toxic': val_labels_stx, 'obscene': val_labels_obs, 'threat': val_labels_thrt, 'insult': val_labels_ins,'identity_hate':val_labels_idthr, 'text':val_texts}, columns=col_names)\n",
    "\n",
    "print(df_trn.head())\n",
    "\n",
    "### creatinig datasets end ###\n",
    "\n",
    "\n",
    "df_trn.to_csv(CLAS_PATH/'train.csv', header=False, index=False)\n",
    "df_val[df_val['toxic']!=-1].to_csv(CLAS_PATH/'test.csv', header=False, index=False)\n",
    "\n",
    "#(CLAS_PATH/'classes.txt').open('w', encoding='utf-8').writelines(f'{o}\\n' for o in CLASSES)\n",
    "\n",
    "trn_texts,val_texts = sklearn.model_selection.train_test_split(\n",
    "    np.concatenate([trn_texts,val_texts]), test_size=0.1)\n",
    "\n",
    "print(len(trn_texts)), print(len(val_texts))\n",
    "\n",
    "df_trn = pd.DataFrame({'text':trn_texts, 'toxic':[0]*len(trn_texts), 'severe_toxic':[1]*len(trn_texts) , 'obscene':[2]*len(trn_texts) , 'threat':[3]*len(trn_texts) , 'insult':[4]*len(trn_texts) , 'identity_hate':[5]*len(trn_texts) }, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'toxic':[0]*len(val_texts), 'severe_toxic':[1]*len(val_texts) , 'obscene':[2]*len(val_texts) , 'threat':[3]*len(val_texts) , 'insult':[4]*len(val_texts) , 'identity_hate':[5]*len(val_texts)}, columns=col_names)\n",
    "\n",
    "df_trn.to_csv(LM_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH/'test.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "print('language model tokens')\n",
    "\n",
    "chunksize=24000\n",
    "\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "\n",
    "    tok = Tokenizer().proc_all(texts, lang = 'en')\n",
    "    return tok, list(labels)\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels\n",
    "\n",
    "df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(LM_PATH/'test.csv', header=None, chunksize=chunksize)\n",
    "\n",
    "tok_trn, trn_labels = get_all(df_trn, 6)\n",
    "tok_val, val_labels = get_all(df_val, 6)\n",
    "\n",
    "(LM_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "\n",
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')\n",
    "\n",
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)\n",
    "\n",
    "max_vocab = 60000\n",
    "min_freq = 2\n",
    "\n",
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)\n",
    "\n",
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))\n",
    "\n",
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))\n",
    "\n",
    "vs=len(itos)\n",
    "\n",
    "print(vs),print(len(trn_lm))\n",
    "\n",
    "print('wikitext103 conversion')\n",
    "\n",
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "\n",
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'\n",
    "\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)\n",
    "\n",
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "\n",
    "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})\n",
    "\n",
    "\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i,w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m\n",
    "\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))\n",
    "\n",
    "\n",
    "print('language model')\n",
    "\n",
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)\n",
    "\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)\n",
    "\n",
    "learner.model.load_state_dict(wgts)\n",
    "\n",
    "lr=1e-3\n",
    "lrs = lr\n",
    "\n",
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)\n",
    "learner.save('lm_last_ft')\n",
    "\n",
    "learner.load('lm_last_ft')\n",
    "\n",
    "learner.unfreeze()\n",
    "\n",
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)\n",
    "\n",
    "print(learner.sched.plot())\n",
    "\n",
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)\n",
    "\n",
    "learner.save('lm1')\n",
    "\n",
    "learner.save_encoder('lm1_enc')\n",
    "\n",
    "print(learner.sched.plot_loss())\n",
    "\n",
    "print('classifier tokens')\n",
    "\n",
    "\n",
    "df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=chunksize)\n",
    "\n",
    "tok_trn, trn_labels = get_all(df_trn, 6)\n",
    "tok_val, val_labels = get_all(df_val, 6)\n",
    "\n",
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
    "np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)\n",
    "\n",
    "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')\n",
    "\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "print(len(itos))\n",
    "\n",
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\n",
    "np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)\n",
    "\n",
    "print('classifier')\n",
    "\n",
    "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
    "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
    "\n",
    "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n",
    "\n",
    "\n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 24\n",
    "\n",
    "min_lbl = trn_labels.min()\n",
    "trn_labels -= min_lbl\n",
    "val_labels -= min_lbl\n",
    "c=int(trn_labels.max())+1\n",
    "\n",
    "\n",
    "trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "val_ds = TextDataset(val_clas, val_labels)\n",
    "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "md = ModelData(PATH, trn_dl, val_dl)\n",
    "\n",
    "\n",
    "# part 1\n",
    "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
    "\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5\n",
    "\n",
    "m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=.25\n",
    "learn.metrics = [accuracy]\n",
    "\n",
    "\n",
    "lr=3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "\n",
    "\n",
    "lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "wd = 1e-7\n",
    "wd = 0\n",
    "learn.load_encoder('lm1_enc')\n",
    "\n",
    "learn.freeze_to(-1)\n",
    "\n",
    "learn.lr_find(lrs/1000)\n",
    "learn.sched.plot()\n",
    "\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "learn.save('clas_0')\n",
    "\n",
    "learn.load('clas_0')\n",
    "\n",
    "learn.freeze_to(-2)\n",
    "\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "learn.save('clas_1')\n",
    "\n",
    "learn.load('clas_1')\n",
    "\n",
    "learn.unfreeze()\n",
    "\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=4, use_clr=(32,10))\n",
    "\n",
    "learn.sched.plot_loss()\n",
    "\n",
    "learn.save('clas_2_new4')\n",
    "\n",
    "learn.load('clas_2_new4')\n",
    "\n",
    "print('fin')\n",
    "\n",
    "predictions = np.argmax(learn.predict(), axis =1 )\n",
    "\n",
    "print(predictions, len(predictions)) \n",
    "acc = (val_labels == predictions).mean()\n",
    "\n",
    "print('Accuracy = ', acc)\n",
    "print('Confusion matrix = ', confusion_matrix(val_labels, predictions))\n",
    "\n",
    "\n",
    " #   predictions = np.argmax(learn.predict(), axis=1)\n",
    " #   acc = (val_lbls_sampled == predictions).mean()\n",
    "\n",
    "\n",
    "#print(learn.sched.plot_loss())\n",
    "\n",
    "\n",
    "########### piecewise model fitting ############\n",
    "\n",
    "trn_labels_tx = trn_labels[:,0]\n",
    "trn_labels_stx = trn_labels[:,1]\n",
    "trn_labels_obs = trn_labels[:,2]\n",
    "trn_labels_thrt = trn_labels[:,3]\n",
    "trn_labels_ins = trn_labels[:,4]\n",
    "trn_labels_idthr = trn_labels[:,5]\n",
    "\n",
    "\n",
    "val_labels_tx = val_labels[:,0]\n",
    "val_labels_stx = val_labels[:,1]\n",
    "val_labels_obs = val_labels[:,2]\n",
    "val_labels_thrt = val_labels[:,3]\n",
    "val_labels_ins = val_labels[:,4]\n",
    "val_labels_idthr = val_labels[:,5]\n",
    "\n",
    "for i in range(6):\n",
    "\n",
    "    print('classifier: ', i )\n",
    " \n",
    "    trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
    "    val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
    "\n",
    "    trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "    val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n",
    "\n",
    "    trn_labels = trn_labels[:,i]\n",
    "    val_labels = val_labels[:,i]\n",
    "\n",
    "    bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "    vs = len(itos)\n",
    "    opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "    bs = 24\n",
    "\n",
    "\n",
    "    min_lbl = trn_labels.min()\n",
    "    trn_labels -= min_lbl\n",
    "    val_labels -= min_lbl\n",
    "    c=int(trn_labels.max())+1\n",
    "\n",
    "\n",
    "    trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "    val_ds = TextDataset(val_clas, val_labels)\n",
    "    trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    md = ModelData(PATH, trn_dl, val_dl)\n",
    "\n",
    "\n",
    "    print(\"# part 1\")\n",
    "    dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
    "\n",
    "    dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5\n",
    "\n",
    "    m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=.25\n",
    "    learn.metrics = [accuracy]\n",
    "\n",
    "\n",
    "    lr=3e-3\n",
    "    lrm = 2.6\n",
    "    lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "\n",
    "\n",
    "    lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "    wd = 1e-7\n",
    "    wd = 0\n",
    "    learn.load_encoder('lm1_enc')\n",
    "\n",
    "    learn.freeze_to(-1)\n",
    "\n",
    "    learn.lr_find(lrs/1000)\n",
    "    learn.sched.plot()\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "    learn.save(f'clas_0_lbl{i}')\n",
    "\n",
    "    learn.load(f'clas_0_lbl{i}')\n",
    "\n",
    "    learn.freeze_to(-2)\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "    learn.save(f'clas_1_lbl{i}')\n",
    "\n",
    "    learn.load(f'clas_1_lbl{i}')\n",
    "\n",
    "    learn.unfreeze()\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=4, use_clr=(32,10))\n",
    "\n",
    "    learn.sched.plot_loss()\n",
    "\n",
    "    learn.save(f'clas_2_new4_lbl{i}')\n",
    "\n",
    "    learn.load(f'clas_2_new4_lbl{i}')\n",
    "\n",
    "    print('fin')\n",
    "\n",
    "    predictions = np.argmax(learn.predict(), axis =1 )\n",
    "\n",
    "    print(predictions, len(predictions)) \n",
    "    acc = (val_labels == predictions).mean()\n",
    "\n",
    "    print('Accuracy = ', acc)\n",
    "    print('Confusion matrix = ', confusion_matrix(val_labels, predictions))\n",
    "\n",
    "print('complete')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
