{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from fastai.text import *\n",
    "import html\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "DATA_PATH=Path('/scratch/ppachigo/3192771/fastai/data/')    ## edit path\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "#! curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "#! tar -xzfv aclImdb_v1.tar.gz -C {DATA_PATH}\n",
    "\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "PATH=Path('/scratch/ppachigo/3192771/fastai/data/toxic/')\n",
    "\n",
    "CLAS_PATH=Path('/scratch/ppachigo/3192771/fastai/data/toxic_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "LM_PATH=Path('/scratch/ppachigo/3192771/fastai/data/toxic_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "########### piecewise model fitting ############\n",
    "\n",
    "trn_labels_tx = trn_labels[:,0]\n",
    "trn_labels_stx = trn_labels[:,1]\n",
    "trn_labels_obs = trn_labels[:,2]\n",
    "trn_labels_thrt = trn_labels[:,3]\n",
    "trn_labels_ins = trn_labels[:,4]\n",
    "trn_labels_idthr = trn_labels[:,5]\n",
    "\n",
    "\n",
    "val_labels_tx = val_labels[:,0]\n",
    "val_labels_stx = val_labels[:,1]\n",
    "val_labels_obs = val_labels[:,2]\n",
    "val_labels_thrt = val_labels[:,3]\n",
    "val_labels_ins = val_labels[:,4]\n",
    "val_labels_idthr = val_labels[:,5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('classifier tokens')\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "print(len(itos))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "\n",
    "    print('classifier forward: ', i )\n",
    " \n",
    "    trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
    "    val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
    "\n",
    "    trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "    val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n",
    "\n",
    "    trn_labels = trn_labels[:,i]\n",
    "    val_labels = val_labels[:,i]\n",
    "\n",
    "    bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "    vs = len(itos)\n",
    "    opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "    bs = 24\n",
    "\n",
    "\n",
    "    min_lbl = trn_labels.min()\n",
    "    trn_labels -= min_lbl\n",
    "    val_labels -= min_lbl\n",
    "    c=int(trn_labels.max())+1\n",
    "\n",
    "\n",
    "    trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "    val_ds = TextDataset(val_clas, val_labels)\n",
    "    trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    md = ModelData(PATH, trn_dl, val_dl)\n",
    "\n",
    "\n",
    "    print(\"# part 1\")\n",
    "    dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
    "\n",
    "    dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5\n",
    "\n",
    "    m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=.25\n",
    "    learn.metrics = [accuracy]\n",
    "\n",
    "\n",
    "    lr=3e-3\n",
    "    lrm = 2.6\n",
    "    lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "\n",
    "\n",
    "    lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "    wd = 1e-7\n",
    "    wd = 0\n",
    "    learn.load_encoder('lm1_enc')\n",
    "\n",
    "    learn.freeze_to(-1)\n",
    "\n",
    "    learn.lr_find(lrs/1000)\n",
    "    learn.sched.plot()\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "    learn.save(f'clas_0_lbl{i}')\n",
    "\n",
    "    learn.load(f'clas_0_lbl{i}')\n",
    "\n",
    "    learn.freeze_to(-2)\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "    learn.save(f'clas_1_lbl{i}')\n",
    "\n",
    "    learn.load(f'clas_1_lbl{i}')\n",
    "\n",
    "    learn.unfreeze()\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=10, use_clr=(32,10))\n",
    "\n",
    "    learn.sched.plot_loss()\n",
    "\n",
    "    learn.save(f'clas_2_lbl{i}')\n",
    "\n",
    "    learn.load(f'clas_2_lbl{i}')\n",
    "\n",
    "    print('fin')\n",
    "\n",
    "    predictions = np.argmax(learn.predict(), axis =1 )\n",
    "\n",
    "    print(predictions, len(predictions)) \n",
    "    acc = (val_labels == predictions).mean()\n",
    "\n",
    "    print('Accuracy = ', acc)\n",
    "    print('Confusion matrix = ', confusion_matrix(val_labels, predictions))\n",
    "\n",
    "print('complete forward')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "\n",
    "    print('classifier backward: ', i )\n",
    "\n",
    "\n",
    "    trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids_bwd.npy')\n",
    "    val_clas = np.load(CLAS_PATH/'tmp'/'val_ids_bwd.npy')\n",
    "\n",
    "    trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "    val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n",
    "     \n",
    "    trn_labels = trn_labels[:,i]\n",
    "    val_labels = val_labels[:,i]\n",
    "\n",
    "\n",
    "    bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "    vs = len(itos)\n",
    "    opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "    bs = 24\n",
    "\n",
    "    min_lbl = trn_labels.min()\n",
    "    trn_labels -= min_lbl\n",
    "    val_labels -= min_lbl\n",
    "    c=int(trn_labels.max())+1\n",
    "\n",
    "\n",
    "    trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "    val_ds = TextDataset(val_clas, val_labels)\n",
    "    trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    md = ModelData(PATH, trn_dl, val_dl)\n",
    "\n",
    "\n",
    "\n",
    "    # part 1\n",
    "    dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])\n",
    "\n",
    "    dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5\n",
    "\n",
    "    m = get_rnn_classifier(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=.25\n",
    "    learn.metrics = [accuracy]\n",
    "\n",
    "\n",
    "    lr=3e-3\n",
    "    lrm = 2.6\n",
    "    lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "\n",
    "\n",
    "    lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "    wd = 1e-7\n",
    "    wd = 0\n",
    "    learn.load_encoder('lm1_enc_bwd')\n",
    "\n",
    "    learn.freeze_to(-1)\n",
    "\n",
    "    learn.lr_find(lrs/1000)\n",
    "    learn.sched.plot()\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "    learn.save(f'clas_0_bwd_lbl{i}')\n",
    "\n",
    "    learn.load(f'clas_0_bwd_lbl{i}')\n",
    "\n",
    "    learn.freeze_to(-2)\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "    learn.save(f'clas_1_bwd_lbl{i}')\n",
    "\n",
    "    learn.load(f'clas_1_bwd_lbl{i}')\n",
    "\n",
    "    learn.unfreeze()\n",
    "\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=10, use_clr=(32,10))\n",
    "\n",
    "    learn.sched.plot_loss()\n",
    "\n",
    "    learn.save(f'clas_2_bwd_lbl{i}')\n",
    "\n",
    "    learn.load(f'clas_2_bwd_lbl{i}')\n",
    "\n",
    "    print('fin')\n",
    "\n",
    "    predictions = np.argmax(learn.predict(), axis =1 )\n",
    "\n",
    "    print(predictions, len(predictions)) \n",
    "    acc = (val_labels == predictions).mean()\n",
    "\n",
    "    print('Accuracy = ', acc)\n",
    "    print('Confusion matrix = ', confusion_matrix(val_labels, predictions))\n",
    "\n",
    "print('complete backward')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
